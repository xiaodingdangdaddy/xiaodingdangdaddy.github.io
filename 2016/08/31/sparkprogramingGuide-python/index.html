<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Spark1.6.2编程指南tuotuo的解读（主要是python相关用法） | 妍兮客栈 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="John Doe">
    
    

    <meta name="description" content="Spark由于从2015年开始才被广泛关注，相关中文学习资料非常缺乏。在这里通过阅读官方的编程指南，学习的同时也做一些相关笔记

概述Spark的主要抽象概念就是分布式弹性数据集RDD(resilient distributed dataset )，数据结构上是一个只读的分区记录集合，可以跨集群节点并行操作。这些集合是弹性的，如果数据集一部分丢失可以对他们进行重建。RDD具有以下特点1234*">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark1.6.2编程指南tuotuo的解读（主要是python相关用法） | 妍兮客栈">
<meta property="og:url" content="http://yoursite.com/2016/08/31/sparkprogramingGuide-python/index.html">
<meta property="og:site_name" content="妍兮客栈">
<meta property="og:description" content="Spark由于从2015年开始才被广泛关注，相关中文学习资料非常缺乏。在这里通过阅读官方的编程指南，学习的同时也做一些相关笔记

概述Spark的主要抽象概念就是分布式弹性数据集RDD(resilient distributed dataset )，数据结构上是一个只读的分区记录集合，可以跨集群节点并行操作。这些集合是弹性的，如果数据集一部分丢失可以对他们进行重建。RDD具有以下特点1234*">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/67a6a651gw1f273wuxzjqj20tp0i0wks.jpg">
<meta property="og:updated_time" content="2016-10-15T17:18:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark1.6.2编程指南tuotuo的解读（主要是python相关用法） | 妍兮客栈">
<meta name="twitter:description" content="Spark由于从2015年开始才被广泛关注，相关中文学习资料非常缺乏。在这里通过阅读官方的编程指南，学习的同时也做一些相关笔记

概述Spark的主要抽象概念就是分布式弹性数据集RDD(resilient distributed dataset )，数据结构上是一个只读的分区记录集合，可以跨集群节点并行操作。这些集合是弹性的，如果数据集一部分丢失可以对他们进行重建。RDD具有以下特点1234*">
<meta name="twitter:image" content="http://ww3.sinaimg.cn/large/67a6a651gw1f273wuxzjqj20tp0i0wks.jpg">
    
    
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">妍兮客栈</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          A site for YanXi Zheng
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">学学习</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">看看图</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">归归档</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/xiaodingdangdaddy" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

    <!-- China social icon -->
    <!--
    
      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>

      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    -->



  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Spark1.6.2编程指南tuotuo的解读（主要是python相关用法）</h1>

    

    <div class="post-meta">
      <time datetime="2016-08-31" class="post-meta__date date">2016-08-31</time> 

      <span class="post-meta__tags tags">

          

          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <blockquote>
<p>Spark由于从2015年开始才被广泛关注，相关中文学习资料非常缺乏。在这里通<br>过阅读<a href="http://spark.apache.org/docs/1.6.2/programming-guide.html" target="_blank" rel="external">官方的编程指南</a>，学习的同时也做一些相关笔记</p>
</blockquote>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark的主要抽象概念就是<strong>分布式弹性数据集RDD</strong>(resilient distributed dataset )，数据结构上是一个只读的分区记录集合，可以跨集群节点并行操作。这些集合是弹性的，如果数据集一部分丢失可以对他们进行重建。RDD具有以下特点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">* RDD只能从持久存储或通过Transformations操作产生，相比于分布式共享内存(DSM)可以更高效实现容错，对于丢失部分数据分区只需根据它的lineage就可重新计算出来，而不需要做特定的Checkpoint。</div><div class="line">* RDD的不变性，可以实现类Hadoop MapReduce的推测式执行。</div><div class="line">* RDD的数据分区特性，可以通过数据的本地性来提高性能，这与Hadoop MapReduce是一样的。</div><div class="line">* RDD都是可序列化的，在内存不足时可自动降级为磁盘存储，把RDD存储于磁盘上，这时性能会有大的下降但不会差于现在的MapReduce。</div></pre></td></tr></table></figure></p>
<p>Spark的另一个抽象概念就是在并行操作中可以共享变量。默认情况下，Spark运行一个函数其实是一组不同节点上的任务集合并行运行，它把函数中用到的变量发送给每一个任务。<br>Spark支持两种共享变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">broadcast variables 可以在所有节点内存中缓存值</div><div class="line">accumulators 只能由叠加得出，比如counters和sums</div></pre></td></tr></table></figure></p>
<h2 id="Spark-链接"><a href="#Spark-链接" class="headerlink" title="Spark 链接"></a>Spark 链接</h2><p>Spark1.6.2需要Python 2.6或者Python 3.4以上版本。可以使用标准CPython解释器,因此像NumPy这样的C函数库是可以用的。另外也支持PyPy 2.3+</p>
<p>用Python来运行Spark程序，可以使用spark-submit脚本。或者使用bin/pyspark来启动一个交互式Python命令行<br>在python程序中，需要输入一些Spark类，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">from pyspark import SparkContext, SparkConf</div></pre></td></tr></table></figure></p>
<p>运行PySpark,驱动节点和工作节点中都需要有满足上面要求的python版本,使用的是PATH中的默认python版本。也可以通过PYSPARK_PYTHON来指定想用的Python版本，比如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ PYSPARK_PYTHON=python3<span class="number">.4</span> bin/pyspark</div><div class="line">$ PYSPARK_PYTHON=/opt/pypy<span class="number">-2.5</span>/bin/pypy bin/spark-submit examples/src/main/python/pi.py</div></pre></td></tr></table></figure></p>
<h2 id="Spark-初始化"><a href="#Spark-初始化" class="headerlink" title="Spark 初始化"></a>Spark 初始化</h2><p>Spark程序第一步要做的就是创建一个SparkContext对象，告诉Spark如何去接入一个集群。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">conf = SparkConf().setAppName(appName).setMaster(master)</div><div class="line">sc = SparkContext(conf=conf)</div></pre></td></tr></table></figure></p>
<p><strong>appName</strong>参数是你的程序在集群界面上显示的名称。<br><strong>master</strong>可以是Spark,Mesos或者YARN集群的URL，也可以是字符串<strong>“local”</strong>表示运行在本地模式。在实际应用中，很少会把master硬编码，而是通过spark-submit的参数来接收。</p>
<h2 id="Spark-shell"><a href="#Spark-shell" class="headerlink" title="Spark shell"></a>Spark shell</h2><p>在PySpark shell启动时候，一个特殊的解释器感知 SparkContext就会自动创建出来，也就是变量<strong>sc</strong>。<br>可以使用增强型的python解释器<strong>IPYTHON</strong>.需要IPYTHON 1.0.0和以上版本。通过以下配置，可以在需要用bin/pyspark的时候用ipython代替：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark</div></pre></td></tr></table></figure></p>
<h2 id="分布式弹性数据集-RDDs"><a href="#分布式弹性数据集-RDDs" class="headerlink" title="分布式弹性数据集(RDDs)"></a>分布式弹性数据集(RDDs)</h2><p>概念在文章《Spark1.6.2 学习官方文档spark-submit相关内容》有介绍。</p>
<h3 id="并行的数据集-Parallelized-collections"><a href="#并行的数据集-Parallelized-collections" class="headerlink" title="并行的数据集(Parallelized collections)"></a>并行的数据集(Parallelized collections)</h3><p>下面是使用SparkContext的<strong>paralleize</strong>函数创建一个包含数字1到5的并行数据集合的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data = [1, 2, 3, 4, 5]</div><div class="line">distData = sc.parallelize(data)</div></pre></td></tr></table></figure></p>
<p>一旦创建，例子中的分布式数据集distData就可以被并行操作。<br>并行数据集的一个重要参数，就是数据集切分的partition的数目。Spark会为集群中每一个partition运行一个任务。典型情况下，为集群中每一个cpu分2-4个partition. Spark会尝试根据你的集群状况来设置partition的数目。但是，可以手动设置parallelize的第二个参数，来设置partition的数目。比如：<strong>sc.parallelize(data,10)</strong></p>
<h3 id="外部数据集"><a href="#外部数据集" class="headerlink" title="外部数据集"></a>外部数据集</h3><p>PySpark 可以将Hadoop支持的所有数据存储类型创建成数据集。包括本地数据、HDFS、Cassandra、HBase、Amazon S3。<br>Spark支持文本文件、序列文件和其他Hadoop 输入格式。<br>可以使用SparkContext的<strong>textFile</strong>函数来创建文本文件RDDs。该函数获取文件的URI（可以是本地路径，或者hdfs://,s3n://等），然后读取并采集每一行，下面是个调用的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; distFile = sc.textFile(&quot;data.txt&quot;)</div></pre></td></tr></table></figure></p>
<p>创建出来该RDDs后，就可以用各种数据操作方法处理。例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">distFile.map(lambda s:len(s)).reduce(lambda a,b:a+b)</div></pre></td></tr></table></figure></p>
<p>一些Spark读取文件的要点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">如果使用本地文件系统作为文件路径，该文件路径必须对所有其他worker一致。因此你可以把该文件拷贝到其他所有workers或者使用网络共享文件系统。</div><div class="line">Spark的所有基于文件输入的函数，包括textFile，都支持文件夹、压缩文件和通配符。比如你可以使用**textFile(&quot;/my/directory&quot;)、textFile(&quot;/my/directory/*.txt&quot;)textFile(&quot;/my/directory/*.gz&quot;)**</div><div class="line">textFile函数可以输入第二个参数来控制文件partitions的数目。默认情况下，Spark为文件的每一个block创建一个partitions(在HDFS中blocks默认为64M)</div></pre></td></tr></table></figure></p>
<p>除了文本文件，Spark的Python API还支持其他几种数据格式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">SparkContext.whoTextFiles可以读取一个文件夹中包含的多个小文本文件，并且为每一个文件返回一对(filename,content)</div><div class="line">RDD.saveAsPickleFile和RDD.pickleFile支持由多个pickled Python对象组成的简单形式生成RDD。pickle序列化使用批处理，每一批默认大小是10</div><div class="line">SequenceFile和Hadoop input/output format</div></pre></td></tr></table></figure></p>
<h3 id="存储和载入SequenceFiles"><a href="#存储和载入SequenceFiles" class="headerlink" title="存储和载入SequenceFiles"></a>存储和载入SequenceFiles</h3><p>和文本文件一样，序列化文件也可以通过指定文件路径来保存和载入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, &quot;a&quot; * x ))</div><div class="line">rdd.saveAsSequenceFile(&quot;path/to/file&quot;)</div><div class="line">sorted(sc.sequenceFile(&quot;path/to/file&quot;).collect())</div><div class="line">[(1, u&apos;a&apos;), (2, u&apos;aa&apos;), (3, u&apos;aaa&apos;)]</div></pre></td></tr></table></figure></p>
<h2 id="RDD-操作"><a href="#RDD-操作" class="headerlink" title="RDD 操作"></a>RDD 操作</h2><p>RDDs支持两种操作：<br><strong>transformations</strong>，从已有数据集的创建新的数据集,例如map;<br><strong>Action</strong>，在数据集上运算后返回值给driver program,例如reduce;<br>注意<strong>reduceByKey</strong>属于transformation，返回的是数据集</p>
<p>Spark中transformation是<strong>惰性的</strong>，在这个过程中仅仅是记录下数据集上应用的transformation。只有Action时候，才真正开始运算</p>
<p>默认情况下，每次执行Action操作，transformed RDD都要重新运算一遍。可以使用<strong>persist(or cache)</strong>函数来把RDD持久化存储在内存中</p>
<h3 id="RDD-基础"><a href="#RDD-基础" class="headerlink" title="RDD 基础"></a>RDD 基础</h3><p>在下面的简单程序中：由于Spark的惰性运算，只有在运行到reduce这个Action<br>动作时候，任务才开始真正运行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">lines = sc.textFile(<span class="string">"data.txt"</span>)</div><div class="line">lineLengths = lines.map(<span class="keyword">lambda</span> s: len(s))</div><div class="line">totalLength = lineLengths.reduce(<span class="keyword">lambda</span> a, b: a + b)</div></pre></td></tr></table></figure></p>
<p>如果接下来还要使用lineLengths，那需要在reduce前添加persist函数，如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">lines = sc.textFile(<span class="string">"data.txt"</span>)</div><div class="line">lineLengths = lines.map(<span class="keyword">lambda</span> s: len(s))</div><div class="line">lineLengths.persist()  <span class="comment"># 可以将该RDD对象在运算后存储在内存中</span></div><div class="line">totalLength = lineLengths.reduce(<span class="keyword">lambda</span> a, b: a + b)</div></pre></td></tr></table></figure></p>
<h3 id="为Spark传函数"><a href="#为Spark传函数" class="headerlink" title="为Spark传函数"></a>为Spark传函数</h3><p>Spark的API 在驱动程序运行在集群上时候非常依赖传递的函数。有三种我们推荐使用的函数：<br>1.Lambda表达式。对于简单的函数可以写成lambda表达式。需要注意的是Lambdas不支持muti-statemenet函数，也不支持无返回值的statements.<br>2.较长代码中，Spark调用的本地内部defs<br>3.模块中的高级别函数</p>
<p>比较一下下面两种map调用方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">class MyClass(object):</div><div class="line">    def __init__(self):</div><div class="line">        self.field = &quot;Hello&quot;</div><div class="line">    def doStuff(self, rdd):</div><div class="line">        return rdd.map(lambda s: self.field + s)</div></pre></td></tr></table></figure></p>
<p>和<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">class MyClass(object):</div><div class="line">    def __init__(self):</div><div class="line">        self.field = &quot;Hello&quot;</div><div class="line">    def doStuff(self, rdd):</div><div class="line">        field = self.field</div><div class="line">        return rdd.map(lambda s: field + s)</div></pre></td></tr></table></figure></p>
<p>前者因为在rdd.map()中调用了self.field，会使得整个MyClass对象发送到集群中。<br>而后者，聪明的在map()外面将self.field复制给了field，成为了本地变量，然后在map()调用时无需前者那么大的代价。<br>下图也是同样的道理：<br><img src="http://ww3.sinaimg.cn/large/67a6a651gw1f273wuxzjqj20tp0i0wks.jpg" alt="传递函数"><br>右边的方式可以避免将整个对象拷贝到闭包中</p>
<h3 id="理解闭包"><a href="#理解闭包" class="headerlink" title="理解闭包"></a>理解闭包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">注意：千万不要在spark中像下面这样coding</div><div class="line">counter = 0</div><div class="line">rdd = sc.parallelize(data)</div><div class="line"></div><div class="line">def increment_counter(x):</div><div class="line">    global counter</div><div class="line">    counter += x</div><div class="line">rdd.foreach(increment_counter)</div></pre></td></tr></table></figure>
<p>在Spark中上面的代码，在local和cluster模式运行时候，结果是完全不同的。</p>
<hr>
<p><strong>local模式</strong>运行。那么这就是一段经典的演示“闭包”概念的代码。函数increment_counter是一个闭包，它有一个自由变量counter, 当这个闭包传递给rdd参与运算的过程中，counter变量一直存在于伴随这个闭包的上下文中，其状态（值）会持续的被改变并保持，因此，在运算结束后，counter的值就是rdd中数据条目的总数。</p>
<p><strong>cluster模式（多节点）</strong>运行。情况就大不相同了，由于数据被分散到了多个节点上并行处理，计算逻辑，也就是我们的闭包也要分发到各个节点上去执行（分发前，Spark会完成闭包的序列化工作），OK，我们可以确定的是：当闭包分发到各个executor上时，闭包中所有自由变量的值都来自于driver发分发时的那个上下文里，现在闭包完全地独立出来了，就是完全在一个全新的环境中运行了，executor本地的自用变量与driver上的那些对应变量已经没有任何关系了，它们都将在各自的环境里自由演变了，彼此之间没有任何联系了。简单地解释就是：闭包和它的上下文只在一个进程（节点）中有效。因此：在本例中，你不能指望在driver端执行这段代码后得到counter的值，实际上它只是在初值是0的时候被复制了n份传递给executor之外，它在driver端什么也没做，而executor端的那个counter副本只在exexutor本地计算着“局部”的counter, 它已和driver端的这个counter没有丝毫的关系，所以最后的结果是：程序执行结束后，打印的这个counter是driver端的这个coutner,它没有被计算，它的值一直是0.</p>
<p>总结一下，如果counter是一个全局性（正对整个程序或着说集群）的只读的初值，那么作为闭包的自由变量传播到整个集群这是有意义的，但如果executor企图要改写一个全局性的初值，则这样做是没有意义的。对于Spark来说，合适作为闭包的自由变量应该是这样一种值：在<strong>局部</strong>数据集计算过程中需要持续追踪状态的变量！<br>(闭包理解引用了bluishglc技术博客上的部分内容，<a href="http://blog.csdn.net/bluishglc/article/details/50715879" target="_blank" rel="external">原文链接</a>)</p>
<hr>
<p>如果确实需要这种全局变量，分发到各节点执行后，能获取想要的改变后的值，可以使用<strong>Accumulator</strong>。Spark中Accumulators提供了一种在分发给各个工作节点中执行后可以安全更新全局变量的机制。</p>
<h3 id="打印RDD元素"><a href="#打印RDD元素" class="headerlink" title="打印RDD元素"></a>打印RDD元素</h3><p>人们通常习惯使用rdd.foreach(println)或者rdd.map(println)这种错误的方式来打印出RDD的元素。说其错误，因为这种方式仅仅适用于本地单机模式。在集群模式下，stdout的输出只是在工作节点上执行，本机driver只负责把这样的任务分发给工作节点，自己没有执行。所以本机并不会打印出想要的结果。<br>一种解决方法是调用<strong>collect()</strong>把RDD复制到driver节点上，像这样<strong>rdd.collect().foreach(println)</strong>。但是，这样又产生了新的问题，由于collect()把整个RDD取到本地的单一机器上，可能会导致内存溢出。<br>因此，<strong>最好最安全的策略</strong>是：如果你需要打印RDD的一部分元素，调用<strong>take()</strong>,如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rdd.take(100).foreach(println)</div></pre></td></tr></table></figure></p>
<p>###Working with Key-Value Pairs<br>Spark在RDDs上的操作对象包括多种类型，有一些RDDs上的特别操作仅仅支持<strong>Key-Value对</strong>。最普遍的是分布式shuffle操作，比如按照Key来grouping或者aggregating元素。<br>下面的代码用<strong>reduceByKey</strong>来计算文件中每一行内容出现的次数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">lines = sc.textFile(&quot;data.txt&quot;)</div><div class="line">pairs = lines.map(lambda s: (s, 1))</div><div class="line">counts = pairs.reduceByKey(lambda a, b: a + b)</div></pre></td></tr></table></figure></p>
<p>可以用<strong>counts.sortByKey()</strong>来将(u’string’,1)这样的键值对按字母顺序排序。最后，使用<strong>counts.collect()</strong>把结果作为一系列对象返回给driver program</p>
<hr>
<h3 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h3><p>下面的表格列出了Spark支持的<strong>transformations</strong><br>更详细的内容，参照官方RDD API(python)–<a href="http://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD" target="_blank" rel="external">pyspark package</a></p>
<hr>
<p><strong>map(func):</strong><br>将每一条源数据通过函数func处理后，形成新的distributed dataset并返回<br><strong>flatMap(func):</strong><br>和map类似，但是每一条输入的源数据，处理后输出项中可以有多个元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])</div><div class="line">&gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())</div><div class="line">[1, 1, 1, 2, 2, 3]</div><div class="line">&gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())</div><div class="line">[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]</div></pre></td></tr></table></figure></p>
<p><strong>mapPartitions(func):</strong><br>和map类似，但是func必须是Iterator,而且func作用于根据partition划分的多<br>个数据上，并返回partition数目的值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2) #第二个参数表明要划分两部分</div><div class="line">&gt;&gt;&gt; def f(iterator): yield sum(iterator)</div><div class="line">&gt;&gt;&gt; rdd.mapPartitions(f).collect()</div><div class="line">[3, 7]   #rdd中的[1,2,3,4],被划分成[1,2]和[3,4],分别作用到f上的到[3,7]</div></pre></td></tr></table></figure></p>
<p><strong>mapPartitionsWithIndex(func)</strong><br>同mapPartitions函数一样，func需要是Iterator，func作用于每个分区，但是跟踪的是每个分区的原始索引，每个func的返回值也是分区的索引<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)</div><div class="line">&gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex</div><div class="line">&gt;&gt;&gt; rdd.mapPartitionsWithIndex(f).sum()</div><div class="line">6  #rdd 分四个区[1],[2],[3],[4],原始索引分别是0,1,2,3 因此最终加起来的结果是6.若第一行改成分2个区，也就sc.parallelize([1, 2, 3, 4], 2),那最后的结果是两个分区的原始索引0+1=2</div></pre></td></tr></table></figure></p>
<p><strong>filter(func):</strong><br>那些输入函数func，使得func返回值为真的源数据，组成新的dataset，并返回<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])</div><div class="line">&gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()</div><div class="line">[2, 4]</div></pre></td></tr></table></figure></p>
<p><strong>sample(withReplacement,fraction,seed=None)</strong><br>Spark的取样函数，参数一True或者False表明取样后放回或者不放回，参数二表示取样的百分比，参数三用于指定随机数生成种子。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(100), 4)</div><div class="line">&gt;&gt;&gt; rdd.sample(False, 0.1, 81).count() </div><div class="line">10</div><div class="line">&gt;&gt;&gt; rdd.sample(False, 0.1, 81).collect()</div><div class="line">[4, 27, 40, 42, 43, 60, 76, 80, 86, 97]</div></pre></td></tr></table></figure></p>
<p>上面例子中表示设定随机数种子为81，不放回抽样10%。最终返回10个结果</p>
<p><strong>union(otherDataset)</strong><br>返回一个并集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd1 = sc.parallelize([1, 1, 2, 3])</div><div class="line">&gt;&gt;&gt; rdd2 = sc.parallelize([2, 3, 4,5])</div><div class="line">&gt;&gt;&gt; rdd1.union(rdd2).collect()</div><div class="line">[1, 1, 2, 3, 2, 3, 4, 5]</div></pre></td></tr></table></figure></p>
<p><strong>intersection(otherDataset)</strong><br>类似union用法，但是返回的是交集</p>
<p><strong>distinct([numTasks]))</strong><br>返回RDD元素中不同的值，也就是重复的元素仅保留一个。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sc.parallelize([1, 1, 2, 2, 2, 3, 3]).distinct().collect()</div><div class="line">[1, 2, 3]</div></pre></td></tr></table></figure></p>
<p><strong>mapValues(f)</strong><br>将f作用到每一对Key-Value上的Value上，不改变Key<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, [&quot;apple&quot;, &quot;banana&quot;, &quot;lemon&quot;]), (&quot;b&quot;, [&quot;grapes&quot;])])</div><div class="line">&gt;&gt;&gt; def f(x): return len(x)</div><div class="line">&gt;&gt;&gt; x.mapValues(f).collect()</div><div class="line">[(&apos;a&apos;, 3), (&apos;b&apos;, 1)]</div></pre></td></tr></table></figure></p>
<p><strong>groupByKey([numTasks])</strong><br>把RDD中的Key-Value对，按照每一个Key，单独组合成一个序列。numTasks指定任务数<br>如RDD中如果有[(“a”, 1),(“b”, 1),(“a”, 1),(“a”, 1),(“b”, 1)],将会返回一个RDD:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[(&apos;a&apos;, &lt;pyspark.resultiterable.ResultIterable object at 0x263f990&gt;), (&apos;b&apos;, &lt;pyspark.resultiterable.ResultIterable object at 0x263fa10&gt;)]</div></pre></td></tr></table></figure></p>
<p>这里面的<strong>pyspark.resultiterable.ResultIterable object</strong>就是按照相同key，组合成的序列。接下来可以调用其他函数对这个序列中的内容进行操作，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1),(&quot;a&quot;, 1),(&quot;b&quot;, 1)])</div><div class="line">&gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(len).collect())</div><div class="line">[(&apos;a&apos;, 3), (&apos;b&apos;, 2)] #计算序列元素个数</div><div class="line">&gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(list).collect())</div><div class="line">[(&apos;a&apos;, [1, 1, 1]), (&apos;b&apos;, [1, 1])] #将序列中的元素以列表形式展示</div></pre></td></tr></table></figure></p>
<p><strong>reduceByKey(func, [numTasks])</strong><br>可以参考groupByKey,只是要把按Key排好的Value序列作用到func上实现reduce,效果相当于上面例子中的rdd.groupByKey().mapValues(len)。<br>需要注意的就是func必须要实现(V,V)=&gt;V。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from operator import add</div><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</div><div class="line">&gt;&gt;&gt; sorted(rdd.reduceByKey(add).collect())</div><div class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</div></pre></td></tr></table></figure></p>
<p><strong>aggregate(zeroValue, seqOp, combOp)</strong><br>看了官方文档上的介绍后，感觉一团浆糊，google一下，终于明白了如何用<br>zeroValue 是整个操作的初始值。<br>seqOp函数作用到RDD数据中的每个分区，并分别得到结果。<br>combOp函数在作用到各个分区的结果上，综合得到最后的结果。在整个计算仅有一个分区的时候 combOp相当于什么都没做。<br>下面用一个例子来说明一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))</div><div class="line">&gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)</div><div class="line">(10, 4)</div><div class="line">#上面seqOp中的y相当于list [1,2,3,4]中的一个,x[0]和x[1]首先被aggregate第一个参数(0,0)赋值</div><div class="line">首先，seqOp起作用，在分区上开始计算</div><div class="line">第一步y=1： 0 + 1，0 + 1 (该步的计算结果(1,1),对应了下一步计算的x[0]和x[1])</div><div class="line">第二步y=2:  1 + 2, 1 + 1</div><div class="line">第三步y=3:  3 + 3, 2 + 1</div><div class="line">第四步y=4:  6 + 4, 3 + 1</div><div class="line">得到结果(10, 4)</div><div class="line">然后combOp起作用，发现只有这一个分区,就什么也不用干了</div></pre></td></tr></table></figure></p>
<p>如果上面改为sc.parallelize([1, 2, 3, 4], 2).aggregate((0, 0), seqOp, combOp)，也就是分成[1,2]和[3,4]两个区，那么这两个区分别像上面那样用seqOp来计算，最终分别得到(3,2)和(7,2),然后在作用到combOp函数上(3+7, 2+2),结果同上也是(10, 4)</p>
<p><strong>sortByKey([ascending], [numTasks])</strong><br>对Key-Value对的RDD进行排序，ascending参数为True时升序，False时降序<br>参数二，表示partition的数目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; tmp = [(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;1&apos;, 3), (&apos;d&apos;, 4), (&apos;2&apos;, 5)]</div><div class="line">&gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 1).collect()</div><div class="line">[(&apos;1&apos;, 3), (&apos;2&apos;, 5), (&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;d&apos;, 4)]</div><div class="line">&gt;&gt;&gt; sc.parallelize(tmp).sortByKey(False, 1).collect()</div><div class="line">[(&apos;d&apos;, 4), (&apos;b&apos;, 2), (&apos;a&apos;, 1), (&apos;2&apos;, 5), (&apos;1&apos;, 3)]</div></pre></td></tr></table></figure></p>
<p><strong>join(otherDataset, [numTasks])</strong><br>(k,v1) join (k, v2),返回(k ,(v1, v2))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; x = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4)])</div><div class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 2), (&quot;a&quot;, 3)])</div><div class="line">&gt;&gt;&gt; sorted(x.join(y).collect())</div><div class="line">[(&apos;a&apos;, (1, 2)), (&apos;a&apos;, (1, 3))]</div></pre></td></tr></table></figure></p>
<p><strong>cogroup(otherDataset, [numTasks])</strong>同 groupWith用法<br>(k,v) cogroup (k, w) 返回一个元组数据集(K, (Iterable<v>, Iterable<w>))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; x=sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 4)])</div><div class="line">&gt;&gt;&gt; y = sc.parallelize([(&quot;a&quot;, 2)])</div><div class="line">&gt;&gt;&gt; x.cogroup(y).collect()</div><div class="line">[(&apos;b&apos;, (&lt;pyspark.resultiterable.ResultIterable object at 0x2d37d10&gt;, &lt;pyspark.resultiterable.ResultIterable object at 0x2cf3ad0&gt;)), (&apos;a&apos;, (&lt;pyspark.resultiterable.ResultIterable object at 0x2cf3210&gt;, &lt;pyspark.resultiterable.ResultIterable object at 0x2cf3850&gt;))]</div></pre></td></tr></table></figure></w></v></p>
<p><strong>cartesian(otherDataset)</strong><br>返回二者的笛卡尔计算结果，Pair类型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; x=sc.parallelize([1, 2, 3])</div><div class="line">&gt;&gt;&gt; y=sc.parallelize([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</div><div class="line">&gt;&gt;&gt; x.cartesian(y).collect()</div><div class="line">[(1, &apos;a&apos;), (1, &apos;b&apos;), (1, &apos;c&apos;), (2, &apos;a&apos;), (2, &apos;b&apos;), (2, &apos;c&apos;), (3, &apos;a&apos;), (3, &apos;b&apos;), (3, &apos;c&apos;)]</div></pre></td></tr></table></figure></p>
<p><strong>pipe(command, [envVars])</strong><br>通过管道使用外部命令command 作用到当前RDD上，然后创建出新的RDD<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; sc.parallelize([&apos;1&apos;, &apos;2&apos;, &apos;&apos;, &apos;3&apos;]).pipe(&apos;cat&apos;).collect()</div><div class="line">[u&apos;1&apos;, u&apos;2&apos;, u&apos;&apos;, u&apos;3&apos;]</div></pre></td></tr></table></figure></p>
<p><strong>glom()</strong><br>合并每一个partition的元素，组合成一个列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)</div><div class="line">&gt;&gt;&gt; sorted(rdd.glom().collect())</div><div class="line">[[1, 2], [3, 4]]</div></pre></td></tr></table></figure></p>
<p><strong>coalesce(numPartitions)</strong><br>将已经分成n个的partition，经过该函数处理再reduce成numPartitions个<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()</div><div class="line">[[1], [2, 3], [4, 5]]</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(2).glom().collect()</div><div class="line">[[1], [2, 3, 4, 5]] #已经分成3个分区，变成2个</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()</div><div class="line">[[1, 2, 3, 4, 5]] #变成1个分区</div></pre></td></tr></table></figure></p>
<p><strong>repartition(numPartitions)</strong><br>把RDD中的数据重新shuffle成numPartition个分区。如果numPartition小于之前的分区数，最好调用上面的coalesce函数，可以避免shuffle操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4) #分成4个分区</div><div class="line">&gt;&gt;&gt; sorted(rdd.glom().collect())</div><div class="line">[[1], [2, 3], [4, 5], [6, 7]]</div><div class="line">&gt;&gt;&gt; rdd.repartition(2).glom().collect() #重新shuffle成2个分区</div><div class="line">[[1, 2, 4, 6], [3, 5, 7]]</div><div class="line">&gt;&gt;&gt; rdd.repartition(10).glom().collect() #重新shuffle成10个分区</div><div class="line">[[5], [1], [], [], [], [6], [2, 7], [3], [], [4]]</div></pre></td></tr></table></figure>
<p><strong>repartitionAndSortWithinPartitions(partitioner)</strong><br>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的</p>
<hr>
<h2 id="Actions"><a href="#Actions" class="headerlink" title="Actions"></a>Actions</h2><p>Spark支持的一些 common actions</p>
<p><strong>reduce(func)</strong><br>func函数输入两个值，返回一个值，新产生的值与RDD中下一个元素再被传入func,直到最后只有一个值为止<br>注意func必须可以commutative and associative，这样才能在并行计算中准确无误。</p>
<p><strong>collect()</strong><br>把RDD中的所有元素以列表的形式返回。通常在数据经过filter或者其他返回足够小数据子集的操作后使用。</p>
<p><strong>count()</strong><br>返回数据集中的元素个数</p>
<p><strong>take(n)</strong><br>返回一个包含数据集前n个元素的数组。</p>
<p><strong>first()</strong><br>返回数据集中的第一个元素，相当于take(1)</p>
<p><strong>takeSample(withReplacement, num, [seed])</strong><br>返回一个从数据集中随机抽取num个元素的数组<br>withReplacement 表示抽取时是否放回。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize(range(0, 10)) #rdd 中共10个元素</div><div class="line">&gt;&gt;&gt; len(rdd.takeSample(True, 20, 1)) #放回抽样方式</div><div class="line">20  #由于是放回抽样，超过rdd元素个数时候依然可以抽取</div><div class="line">&gt;&gt;&gt; len(rdd.takeSample(False, 5, 2))</div><div class="line">5 #抽样5次，得到5个元素的数组</div><div class="line">&gt;&gt;&gt; len(rdd.takeSample(False, 15, 3))</div><div class="line">10 #不放回抽样，抽一个少一个，最终只能抽样10次，得到的数组个数也为10</div></pre></td></tr></table></figure></p>
<p><strong>takeOrdered(n, [ordering])</strong><br>按照升序，或者custom comparator，返回RDD 前n个元素。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)</div><div class="line">[1, 2, 3, 4, 5, 6]</div><div class="line">&gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)</div><div class="line">[10, 9, 7, 6, 5, 4]</div></pre></td></tr></table></figure></p>
<p><strong>saveAsTextFile(path)</strong><br>把数据集中的元素写入文本文件，文本文件路径可以是本地、HDFS或者Hadoop支持的其他文件系统。</p>
<p><strong>countByKey()</strong><br>仅支持(key, value)类型的RDDs使用。计算每一个Key的数目，并返回结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])</div><div class="line">&gt;&gt;&gt; sorted(rdd.countByKey().items())</div><div class="line">[(&apos;a&apos;, 2), (&apos;b&apos;, 1)]</div></pre></td></tr></table></figure></p>
<p><strong>foreach(func)</strong><br>把func函数作用到RDD的每个元素上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; def f(x): print(x)</div><div class="line">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreach(f)</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="Shuffle-operations"><a href="#Shuffle-operations" class="headerlink" title="Shuffle operations"></a>Shuffle operations</h2><p>Spark中的一些operation会引起<strong>shuffle</strong>事件。shuffle是spark中用于把不同partition中数据集合，然后重新分布的机制。由于这会在executors和machines之间拷贝数据，导致shuffle操作非常的复杂和代价高昂。</p>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background**"></a>Background**</h3><p>以reduceByKey为例，一个Key所有的value可能分布于不同的partition上，所以Spark在计算时是需要执行一个All-To-All的操作，也就是在所有分区上找到所有Key对应的所有Value，针对某一个key,需要把它对应的所有的value从对应的partition上复制回来进行计算， 这个过程就是shuffle<br>Spark中可能引发shuffle的operation主要有：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">repartition类型： repartition 和coalesce</div><div class="line">ByKey  类型：     groupByKey 和reduceByKey</div><div class="line">join   类型：     cogroup和jpin</div><div class="line">所有这些，都是需要跨分区操作的</div></pre></td></tr></table></figure></p>
<h3 id="Performance-Impact"><a href="#Performance-Impact" class="headerlink" title="Performance Impact"></a>Performance Impact</h3><p>由于牵扯到磁盘I/O，序列化数据和网络I/O。在shuffle中，为了组织数据，Spark会产生多组任务：map tasks to organize the data, and a set of reduce tasks to aggregate it.这其中大量的map任务结果会保存在内存中，对内存资源带来很大负担，而内存不足时候，这些map结果又会被持久化存储到硬盘上，带来大量的磁盘I/O。shuffle还会产生大量中间文件，对于长期运行的Spark job,会占用大量的硬盘空间。</p>
<h2 id="RDD-Persistence"><a href="#RDD-Persistence" class="headerlink" title="RDD Persistence"></a>RDD Persistence</h2><p>Spark的重要能力之一就是persisting (or caching) a dataset in memory across operations。当RDD被持久化存储后，所有存储其部分数据的节点，都可以在内存里计算，或者丢失后恢复。这样使得后续使用这部分RDD时候更加快速(often by more than 10x)。对迭代算法和快速交互场景下，Caching是非常关键的。<br>通过persist()或者cache()来使得RDD被持久化存储。Spark的cache具有容错性，RDD部分丢失后，会自动使用原始创建它的transformations来重新计算，并恢复。<br>RDD持久化存储有不同的storage level,你可以选择存储在磁盘、内存或者以序列化的JAVA对象形式保存。<br>对Python来说，存储对象总是使用<strong>Pickle库</strong>来序列化的。<br>Spark自动监视cache在每个节点的使用，并通过least-recently-used (LRU) fashion来删除旧的数据部分。当然也可以使用RDD.unpersist()来主动删除</p>
<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>##Broadcast Variables##<br>用法官方定义的很精辟：The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.<br>通过调用SparkContext.broadcast(v)创建出一个Broadcast变量。它的值可以调用value方法被提取，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])</div><div class="line">&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;</div><div class="line"></div><div class="line">&gt;&gt;&gt; broadcastVar.value</div><div class="line">[1, 2, 3]</div></pre></td></tr></table></figure></p>
<p>##Accumulators##<br>An accumulator is created from an initial value v by calling SparkContext.accumulator(v). Tasks running on the cluster can then add to it using the add method or the += operator (in Scala and Python). However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.<br>accumulator并不会影响Spark的Lazy机制，也 就是说，在一系列的transformation操作之后 ，accumulator的值未必会被更改，而是直到一个action执行之后，它的值在driver端才有可能更新，比如像下面这个例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">accum = sc.accumulator(0)</div><div class="line">rdd = sc.parallelize([1,2,3,4])</div><div class="line">def f(x):</div><div class="line">    global accum</div><div class="line">    accum += x</div><div class="line">rdd.foreach(f)</div><div class="line">accum.value   # 10</div></pre></td></tr></table></figure></p>

  </section>

  
  

<section class="post-comments">

    <div class="ds-thread" data-thread-key="2016/08/31/sparkprogramingGuide-python/"></div>

    <script type="text/javascript">
      var duoshuoQuery = {short_name:"xiaodingdangdaddy"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
        || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script> 

</section>


</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
